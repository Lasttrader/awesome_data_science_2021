{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"15.1 Обучение с подкреплением","provenance":[{"file_id":"1O7PO3ko2qs87kRUvcr6hVdvVFdaN5MVy","timestamp":1574769103405},{"file_id":"16v1ZulyIGTb7bw0DD939H7aWKZ_2ZZ4i","timestamp":1574332444115},{"file_id":"12_tKJdCfQOqct6ehoeWWmBDToQg1AbAu","timestamp":1571428026561}],"collapsed_sections":["AajzftiKybYf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SnBDolSeL618"},"source":["# **Обучение с Подкреплением**"]},{"cell_type":"markdown","metadata":{"id":"Up2dqyHR8j4x"},"source":["## Import библиотек"]},{"cell_type":"code","metadata":{"id":"ZIQoQnyUdjt4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhi9xGpnhEji","executionInfo":{"status":"ok","timestamp":1626155146145,"user_tz":-180,"elapsed":321,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["import gym #загружаем \"тренажер\" Gym из платформы OpenAi, предоставляющий среду для работы над обучением с подкреплением \n","import numpy as np #импортируем библиотеку для работы с массивами данных\n","import keras #импортируем нейросетевую библиотеку \n","from tensorflow.keras.models import Model, load_model #из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n","from tensorflow.keras.layers import Dense, Flatten, Input, Multiply, Lambda, Conv2D, MaxPooling2D, Reshape #из кераса загружаем необходимые слои для нейросети\n","from tensorflow.keras.optimizers import RMSprop #из кераса загружаем выбранный оптимизатор\n","import time #модуль для операций со временными характеристиками\n","\n","import matplotlib.pyplot as plt #импортируем библиотеку для визуализации данных\n","#\"магическая\"команда python для запуска библиотеки в ноутбуке\n","%matplotlib inline "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWmqjSMve32l"},"source":["## Взгляд на игру"]},{"cell_type":"code","metadata":{"id":"LSs7en3kh_x0","colab":{"base_uri":"https://localhost:8080/","height":358},"executionInfo":{"status":"error","timestamp":1626155149643,"user_tz":-180,"elapsed":364,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}},"outputId":"392bc0fd-7407-420c-f2fe-46d71cca2164"},"source":["env = gym.make('Pong-v0') #создаем среду игры Понг средствами OpenAI Gym('env' = environment)\n","observation = env.reset() #задаем начальное состояние среды, которое наблюдает агент\n","observation.shape #взглянем на форму состояния среды\n","#увидим, что это изображение размером 210*160 с тремя RGB каналами"],"execution_count":8,"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-b489f4e7f91e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pong-v0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#создаем среду игры Понг средствами OpenAI Gym('env' = environment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#задаем начальное состояние среды, которое наблюдает агент\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m#взглянем на форму состояния среды\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#увидим, что это изображение размером 210*160 с тремя RGB каналами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Make the environment aware of which spec it came from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_game_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_difficulty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdifficulty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/atari_py/games.py\u001b[0m in \u001b[0;36mget_game_path\u001b[0;34m(game_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_games_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".bin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ROM is missing for %s, see https://github.com/openai/atari-py#roms for instructions'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: ROM is missing for pong, see https://github.com/openai/atari-py#roms for instructions"]}]},{"cell_type":"code","metadata":{"id":"yjP4qmiody-O"},"source":["# В случае ошибки с ROM, загрузить и импортировать ROMs, как показано в ссылке выше."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUjyJkr6iDx0","executionInfo":{"status":"aborted","timestamp":1626155134397,"user_tz":-180,"elapsed":24,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["plt.imshow(observation) #вызовем визуализацию состояния среды с помощью метода imshow от matplotlib.pyplot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJis251BiFxo","executionInfo":{"status":"aborted","timestamp":1626155134397,"user_tz":-180,"elapsed":24,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# В понге:\n","# 0 цифровое обозначение если ракетка 'остаётся на месте'\n","# 2 цифровое обозначение для действия ракеткой 'сдвинуться вверх'\n","# 3 цифровое обозначение для действия ракеткой 'сдвинуться вниз'\n","\n","# Проиграем вперёд игру на 30 кадров чтобы увидеть полную сцену игры\n","for i in range(100):\n","    observation, reward, done, info = env.step(0) #0 обозначает,что ничего не делаем, остаемся на месте\n","    plt.imshow(observation) #взглянем на текущее состояние среды: наш агент владеет зелёной ракеткой, появился соперник и мяч\n","    plt.show()\n","# step - такт, шаг в игре: принимает (action, действие агента), возвращает кортеж (observation, reward, done, info)\n","# observation (object) - текущее состояние среды, которое наблюдает агент(пиксели)\n","# reward (float) - награда за совершённое действие\n","# done (True or False) - обозначает завершился ли игровой эпизод (в понге до победы 21 очко)\n","# info (dict) - вспомогательная диагностическая информация, типа кол-во оставшихся жизней(неактуально для понга)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQcIp2WlnhiI"},"source":["СЛЕДУЮЩИЙ КАДР"]},{"cell_type":"code","metadata":{"id":"wZhsLc87iQDy","executionInfo":{"status":"aborted","timestamp":1626155134398,"user_tz":-180,"elapsed":25,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Policy(политику, стратегию агента) будет позже задавать нейросеть.\n","# мы подадим ей разницу между новым и предыдущим кадром, чтобы отслеживалось смещение мяча/ракеток и данные были понятными\n","newObservation, reward, done, info = env.step(2) #задаем очередным шагом следующий кадр, при этом смещая ракетку вверх\n","plt.imshow(newObservation) #взглянем на следующий кадр(нам смещение едва видно, либо не видно, т.к визуально длина шага мала)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THb6p7QilI4k"},"source":["## Предобработка данных"]},{"cell_type":"code","metadata":{"id":"NY4wrglpiXxQ","executionInfo":{"status":"aborted","timestamp":1626155134398,"user_tz":-180,"elapsed":25,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Зададим функцию предобработки данных для подачи в нейросеть\n","# избавимся вот всего лишнего(фон, размер и т.п), оставив на экране лишь мяч и ракетки\n","def preprocessFrames(newFrame,lastFrame): #подаем в функцию новый и предыдущий кадр\n","  nFrame = newFrame.astype(np.int32) #переводим в целочисленный тип новый кадр\n","  nFrame[nFrame==144] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  nFrame[nFrame==109] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  lFrame = lastFrame.astype(np.int32) #переводим в целочисленный тип предыдущий кадр\n","  lFrame[lFrame==144] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  lFrame[lFrame==109] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  deltaFrame = nFrame - lFrame #задаём разницу между новым и предыдущим кадром \n","  # Отрезаем верхнюю и нижнюю(неинформативные) части экрана, по 35 сверху и снизу\n","  deltaFrame = deltaFrame[35:195] #срезали края по высоте, осталась картинка 160*160\n","  # Делаем сжатие кадра в 2 раза по обеим сторонам изображения и оставляем монотонный канал \n","  deltaFrame=deltaFrame[::2,::2, 0]\n","  # Масштабирование чисел от 0 до 1\n","  maxValue = deltaFrame.max() if deltaFrame.max()> abs(deltaFrame.min()) else abs(deltaFrame.min())\n","  if maxValue != 0:\n","      deltaFrame=deltaFrame/maxValue\n","  return deltaFrame #функция вернет разницу между кадрами в оптимальном виде"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIg0pSS3iaQO","executionInfo":{"status":"aborted","timestamp":1626155134399,"user_tz":-180,"elapsed":25,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["plt.imshow(preprocessFrames(newObservation, observation), plt.cm.gray) #выведем результат предобработки наших кадров\n","#смещение здесь - это шаг от черного пикселя к светлому "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW5ADGTuijHU","executionInfo":{"status":"aborted","timestamp":1626155134399,"user_tz":-180,"elapsed":25,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["preprocessFrames(newObservation, observation) #выведем массив для этого состояния среды(кадра)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0dkdm0OimmQ","executionInfo":{"status":"aborted","timestamp":1626155134400,"user_tz":-180,"elapsed":26,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["preprocessFrames(newObservation, observation).shape #выведем форму массива"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBhBpQMWiK7V"},"source":["## Моделируем нейросеть"]},{"cell_type":"code","metadata":{"id":"u0BAA4yzGKhw","executionInfo":{"status":"aborted","timestamp":1626155134400,"user_tz":-180,"elapsed":26,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Cостоянием среды будет картинка 80*80, полученная вычитанием двух последовательных кадров, где по итогу все будет...\n","# ...заполнено нулями, а в местах смещения мяча либо ракетки - ненулевые значения.\n","# Далее keras'ом создадим policy, которая на основе состояния(картинки) выбирает действия.\n","# Output сети - вероятность того что нужно двигаться вверх"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zCF5fYbin8v","executionInfo":{"status":"aborted","timestamp":1626155134401,"user_tz":-180,"elapsed":26,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Простая модель с двумя слоями, на 200 нейронов в скрытом слое и сигмоидой на выходе\n","inputs = Input(shape=(80,80)) #на входном слое сетки изображение 80*80\n","flattenedLayer = Flatten()(inputs) #перевели в вектор\n","fullConnected = Dense(units=200, activation='relu', use_bias=False)(flattenedLayer) #задали 200 нейронов и активацию релу\n","sigmoidOutput = Dense(1, activation='sigmoid', use_bias=False)(fullConnected) #сигмоида на выходе\n","# POLICY\n","policyNetworkModel = Model(inputs=inputs, outputs=sigmoidOutput) # собрали модель стратегии(Model - абстрактный класс базовой модели)\n","policyNetworkModel.summary() #посмотрим на модель"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ehKWV5P4Mvn"},"source":["#### Conv2D слой"]},{"cell_type":"code","metadata":{"id":"t7xPKM4SM505","executionInfo":{"status":"aborted","timestamp":1626155134401,"user_tz":-180,"elapsed":26,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Простая модель с двумя слоями, на 200 нейронов в скрытом слое и сигмоидой на выходе\n","inputs = Input(shape=(80,80)) #на входном слое сетки изображение 80*80\n","layer = Reshape((80,80,1))(inputs)\n","layer = Conv2D(32, (3,3), padding='same')(layer)\n","layer = Conv2D(32, (3,3), padding='same')(layer)\n","layer = MaxPooling2D(2)(layer)\n","flattenedLayer = Flatten()(layer) #перевели в вектор\n","fullConnected = Dense(units=32, activation='relu', use_bias=False)(flattenedLayer) #задали 200 нейронов и активацию релу\n","sigmoidOutput = Dense(1, activation='sigmoid', use_bias=False)(fullConnected) #сигмоида на выходе\n","policyNetworkModel = Model(inputs=inputs, outputs=sigmoidOutput) # собрали модель стратегии(Model - абстрактный класс базовой модели)\n","policyNetworkModel.summary() #посмотрим на модель"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEw1xGHWaCwG"},"source":["##Задаем функцию потерь в соответствии с вознаграждением"]},{"cell_type":"code","metadata":{"id":"vc8WJxR7iv9_","executionInfo":{"status":"aborted","timestamp":1626155134402,"user_tz":-180,"elapsed":27,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# episode - одна тренировочная игра от начала партии до перезагрузки (розыгрыш эпизода до 21 очка; после победы или поражения вызываем reset)\n","episodeReward = Input(shape=(1,), name='episodeReward') #задаем награду за эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lOiOSUYxi4Au","executionInfo":{"status":"aborted","timestamp":1626155134402,"user_tz":-180,"elapsed":27,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","#Функция потерь в керасе имеет вид def loss(yTrue,yPred):...А так как нам нужно включить reward в loss, создаем...\n","#...функцию rewardedLoss поверх неё, чтобы на входе добавить episodeReward\n","###########################\n","def rewardedLoss(episodeReward): #задаем новую функцию потерь, принимающую episodeReward, награда\n","  def loss(yTrue,yPred):\n","    # подаём в кач-ве yTrue фактически сделанное действие(action) \n","    # если фактически сделанное действие было движением вверх - подаем 1 на yTrue, если нет то подаем 0\n","    # yPred - выход сетки(вероятность выбора движения вверх)\n","    # мы не подаём yPred в нейронку, его вычисляет керас\n","\n","    # сначала log(0) and log(1) неопределены - загоняем yPred между значениями:\n","    tmpPred = Lambda(lambda x: keras.backend.clip(x,0.05,0.95))(yPred)\n","    # вычисляем логарифм вероятности. yPred - вероятность выбора движения вверх \n","    # помним что yTrue = 1 когда фактически выбрано движение вверх, и 0 - когда вниз\n","    # формула похожа на кросс-энтропию в керасе, но здесь мы прописываем её вручную, чтобы умножить на значение награды\n","    tmpLoss = Lambda(lambda x:-yTrue*keras.backend.log(x)-(1-yTrue)*(keras.backend.log(1-x)))(tmpPred)\n","    # обновленная функция потерь - \"функция политики\"\n","    policyLoss = Multiply()([tmpLoss, episodeReward]) #добавляем в loss умножение на награду за эпизод\n","    return policyLoss #ввели обновленную функцию политики\n","  return loss # возвращаем обновленную функцию политики"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EATpSguci8Zz","executionInfo":{"status":"aborted","timestamp":1626155134402,"user_tz":-180,"elapsed":27,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Создаем керас-оптимизатор и нейронку для обучения\n","###########################\n","# episodeReward = Input(shape=(1,), name='episodeReward') #задаем награду за эпизод (задана выше)\n","policyNetworkTrain = Model(inputs=[inputs, episodeReward],outputs=sigmoidOutput) #задаем сеть с добавлением на вход награды\n","\n","myOptimizer = RMSprop(lr=0.0001) #выбрали оптимизатор с заданной скоростью обучения\n","policyNetworkTrain.compile(optimizer=myOptimizer, loss=rewardedLoss(episodeReward)) #задаем сеть с новой функцией потерь policy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEgR3LiLuqUn"},"source":["## Генерация игрового эпизода с участием нейросети"]},{"cell_type":"code","metadata":{"id":"4XGRbvGAjA-F","executionInfo":{"status":"aborted","timestamp":1626155134403,"user_tz":-180,"elapsed":28,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Создадим функцию, которая будет генерировать игровой эпизод с участием нейросети\n","###########################\n","#игра начнется с того, что в новом эпизоде наш агент сделает движение(\"actualAction\") вверх с вероятностью(upProbability), которую предсказала наша...\n","#...сетка policyNetwork, принявшая на вход состояние среды reshapedInput с подбитой под сеть формой. \n","#reshapedInput же получили из состояния processedNetworkInput, которое представляет собой разницу в кадрах между новым и предыдущим...\n","#...кадром, полученную функцией preprocessFrames.\n","\n","\n","def generateEpisode(policyNetwork): #подаем на вход функции модель нейросети\n","  statesList = [] #список состояний в течение эпизода, размер = (x,80,80)\n","  upDownActionList=[] #список движений в течение эпизода: вверх - 1, вниз - 0\n","  rewardsList=[] #список наград за каждое действие\n","  networkOutputList=[] #на выходе нейросети - вероятность что нужно идти вверх; собираем список из вероятности на каждом шаге\n","  env=gym.make(\"Pong-v0\") #cоздали среду\n","  observation = env.reset() #перезагрузили состояние среды\n","  newObservation = observation #получили новое состояние, которое наблюдает агент\n","  done = False #игровой эпизод активен(не завершён)\n","\n","  while done == False: # пока игровой эпизод не завершён\n","    # На вход сети будет подаваться очередное состояние - разница между кадрами. \n","    processedNetworkInput = preprocessFrames(newFrame=newObservation, lastFrame=observation) #зададим это состояние\n","    statesList.append(processedNetworkInput) #добавим в список состояний (впоследствие станет 'x'ом для входа в нейронку)\n","    reshapedInput = np.expand_dims(processedNetworkInput, axis=0) # размер 'x' - (80,80), делаем размерность (x,(1,80,80))\n","\n","    upProbability = policyNetwork.predict(reshapedInput, batch_size=1)[0][0] #задаем вероятность шага вверх\n","    networkOutputList.append(upProbability) #добавляем к списку из вероятности идти вверх на каждом шаге\n","    actualAction = np.random.choice(a=[2,3], size=1, p=[upProbability, 1-upProbability])\n","    # сделаем фактический шаг либо вверх(2) c вероятностью upProbability, либо вниз(3) с обратной вероятностью\n","\n","    if actualAction == 2: #если пошли вверх\n","      upDownActionList.append(1) # добавляем единицу в список движений в течение эпизода\n","    else: #если не пошли вверх\n","      upDownActionList.append(0) #то добавляем ноль\n","\n","    observation = newObservation # текущий newObservation записываем как старый перед тем, как сделать следующий шаг\n","    newObservation, reward, done, info = env.step(actualAction) #сделали новый шаг, получили новую награду, новое состояние\n","\n","    rewardsList.append(reward) #добавили текущую награду в список\n","\n","    if done: #если игровой эпизод закончен\n","      break #завершаем цикл\n","\n","  env.close() #закрываем текущую среду\n","  return statesList, upDownActionList, rewardsList, networkOutputList\n","#функция возвращает: список состояний в течение эпизода,список движений в течение эпизода, список наград за каждое действие,\n","#и список вероятностей того, что нужно идти вверх"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWg_B1B3jCqL","executionInfo":{"status":"aborted","timestamp":1626155134403,"user_tz":-180,"elapsed":27,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Сгенерируем эпизод игры с необученной сетью\n","###########################\n","statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel)\n","print(\"----------------------\")\n","print(\"Количество состояний в эпизоде = \"+str(len(statesList))) #количество состояний (кадров) в эпизоде\n","print(\"Форма состояния \"+str(statesList[0].shape)) #форма каждого состояния\n","print(\"Количество наград за эпизод = \"+str(len(rewardsList))) #количество наград за эпизод(включая нулевые награды)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ox8tCAR56igu"},"source":["#### Пример генерации нскольких эпизодов \\ визуализация"]},{"cell_type":"code","metadata":{"id":"Iky-630VMaqM","executionInfo":{"status":"aborted","timestamp":1626155134404,"user_tz":-180,"elapsed":28,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Генерируем несколько эпизодов\n","for i in range(10):\n","###########################\n","# Сгенерируем эпизод игры с необученной сетью\n","###########################\n","  statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel)\n","  print(\"----------------------\")\n","  print(\"Количество состояний в эпизоде = \"+str(len(statesList))) #количество состояний (кадров) в эпизоде\n","  print(\"Форма состояния \"+str(statesList[0].shape)) #форма каждого состояния\n","  print(\"Количество наград за эпизод = \"+str(len(rewardsList))) #количество наград за эпизод(включая нулевые награды)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqBtkj-ujPjr","executionInfo":{"status":"aborted","timestamp":1626155134404,"user_tz":-180,"elapsed":28,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Взглянем на фрагмент списка из вероятности идти вверх на каждом шаге.\n","print(networkOutputList[50:70])\n","# увидим что на каждом шаге вероятность крутится около 50% - сеть пока не понимает куда лучше шагать"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1bbRHsmjUxn","executionInfo":{"status":"aborted","timestamp":1626155134405,"user_tz":-180,"elapsed":29,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Взглянем на фактические действия: 1 - вверх, 0 - вниз\n","upDownActionList[50:70]\n","# Соответственно шаги также были случайными"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4tGmjzxjWA0","executionInfo":{"status":"aborted","timestamp":1626155134405,"user_tz":-180,"elapsed":29,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["#выведем список наград(почти везде нули, кроме тех моментов когда выигрываем или проигрываем очко)\n","print(rewardsList[50:70])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAUzdwzhjewJ","executionInfo":{"status":"aborted","timestamp":1626155134405,"user_tz":-180,"elapsed":28,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Взглянем на то, сколько мы раз выиграли в течение эпизода:\n","print(\"Количество выигранных очков = \"+str(len(list(filter(lambda r: r>0,rewardsList))))) #берем как длину списка, отфильтрованного по наградам,большим 0\n","print(\"Количество проигранных очков = \"+str(len(list(filter(lambda r: r<0,rewardsList))))) #берем как длину списка, отфильтрованного по наградам,меньшим 0\n","print(\"Количество нулевых наград = \"+str(len(list(filter(lambda r: r==0,rewardsList))))) #с фильтром по 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-dFUNGGjgQh","executionInfo":{"status":"aborted","timestamp":1626155134406,"user_tz":-180,"elapsed":29,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Выведем на графике полученные награды. Ненулевые награды за каждое очко будут отмечены точками.\n","plt.plot(rewardsList, '.') #точки будут наградами на графике\n","ax=plt.gca() #получим текущие оси\n","ax.grid(True) # с фоновой сеткой"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdrN9Jy83-Tp"},"source":["## Эффективно определяем вознаграждение"]},{"cell_type":"code","metadata":{"id":"FClyJs9ZjueI","executionInfo":{"status":"aborted","timestamp":1626155134406,"user_tz":-180,"elapsed":29,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Награды пока присуждаются только тем движениям, в момент после которых было пропущено или забито очко. Но так мы научимся нескоро,\n","#нам нужно указать на важность действия чуть ранее, когда мы отбивали мяч ракеткой. Чтобы сетка поняла что отбивать мяч - это хорошо,\n","#распределим награду на все шаги, предшествующие забитому/пропущеному мячу, причем чем давнее был шаг, тем меньший коэффициент награды он получит.\n","\n","###########################\n","# Сформулируем функцию, которая распределит ненулевую награду для всех шагов в удачном/неудачном розыгрыше\n","###########################\n","def processRewards(rewardList): #подадим в функцию список наград\n","  rewardDecay = 0.99 #установим высший коэффициент награды для ближнего действия к победному/проигрышному действию\n","  tmpReward = 0 #создадим временную переменную для награды\n","  rewardDecayed = np.zeros_like(rewardList,dtype=np.float32) #создадим массив из нулей для нового формата наград\n","  for i in range(len(rewardList)-1,-1,-1): #будем идти в обратную сторону от награды с шагом \"-1\"\n","    if rewardList[i] == 0: #если награда нулевая \n","      tmpReward = tmpReward*rewardDecay #зададим ей коэффициент\n","      rewardDecayed[i] = tmpReward #и добавим в массив\n","    else: #иначе\n","      tmpReward = rewardList[i] #оставим награду неизменной\n","      rewardDecayed[i] = tmpReward #и введём в том же виде в массив\n","  #запустим нормализацию значений наград, что позволит сетке лучше понимать, где были хорошие и плохие шаги\n","  rewardDecayed -= np.mean(rewardDecayed) #вычтем среднее\n","  rewardDecayed /= np.std(rewardDecayed) #разделим на стандартное отклонение\n","  return rewardDecayed\n","#функция возвращает обновленный формат наград"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8VxXQM0jwoq","executionInfo":{"status":"aborted","timestamp":1626155134407,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Посмотрим на распределение наград по новой функции\n","plt.plot(processRewards(rewardsList),'-') #добавляем в график награды с учетом новой функции\n","ax = plt.gca() #получим текущие оси\n","ax.grid(True) #с фоновой сеткой\n","# новое распределение наград даст лучшее отражение эффективности действий"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDCw_N1DGAax"},"source":["## Пример игрового эпизода с последующей тренировкой сети"]},{"cell_type":"code","metadata":{"id":"CT4iDIRJkKME","executionInfo":{"status":"aborted","timestamp":1626155134407,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Сгенерируем новый игровой эпизод:\n","statesList,upDownActionList,rewardsList,networkOutputList = generateEpisode(policyNetworkModel)\n","# сыграли до 21 очка, проиграли, получили списки всех состояний среды(кадров), шагов агента, вознаграждений и предсказаний\n","\n","print(\"Количество состояний среды = \"+str(len(statesList))) #выведем количество состояний среды(кадров) в игровом эпизоде\n","print(\"Форма состояний среды = \"+str(statesList[0].shape)) #выведем форму для каждого состояния\n","print(\"Список наград  = \"+str(len(rewardsList))) #выведем список наград в игровом эпизоде(включая нулевые)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QchFZR-7HJk"},"source":["ДАННЫЕ ДЛЯ ТРЕНИРОВКИ"]},{"cell_type":"code","metadata":{"id":"Hj0O5xrckS_r","executionInfo":{"status":"aborted","timestamp":1626155134408,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# Подготовим данные для тренировки    \n","x = np.array(statesList) # зададим 'x' как массив состояний \n","episodeReward = np.expand_dims(processRewards(rewardsList), 1) #подобьем размер для входа в сетку\n","\n","yTmp = np.array(upDownActionList) #зададим 'y' как список движений вверх(1)/вниз(0)\n","yTrue = np.expand_dims(yTmp, 1) # скорректируем форму под сетку\n","\n","\n","print(\"Форма наград за эпизод =\", episodeReward.shape) #выведем форму наград за эпизод\n","print(\"Форма состояний среды =\", x.shape) #выведем форму состояний среды ('x' для нейросети)\n","print(\"Форма фактических движений =\", yTrue.shape) #выведем форму фактических движений ('y' для нейросети) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nYKeGcAkYqm","executionInfo":{"status":"aborted","timestamp":1626155134408,"user_tz":-180,"elapsed":30,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["# \"Скормим\" модели новые 'x' и 'y'\n","policyNetworkTrain.fit(x=[x, episodeReward], y=yTrue)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AFGjsI6q7OpQ"},"source":["СЕРИЯ ИГРОВЫХ ЭПИЗОДОВ"]},{"cell_type":"code","metadata":{"id":"I0XSt8-Xkgnq","executionInfo":{"status":"aborted","timestamp":1626155134409,"user_tz":-180,"elapsed":31,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Теперь создадим серию игровых эпизодов\n","# Затем предобработаем данные и скормим нейросети\n","###########################\n","def generateEpisodeBatchesTraining(nBatches=10): #подаем на вход заданное количество игровых серий\n","  env = gym.make('Pong-v0') #создаем игровую среду Pong\n","  batchStateList = [] #зададим серию списков состояний\n","  batchUpDownActionList = [] #зададим серию списков движений\n","  batchRewardsList = [] #зададим серию списков наград\n","  batchNetworkOutputList = [] #зададим серию списков из вероятности идти вверх\n","  # генерим 10 игр\n","  for i in range(nBatches): #для каждой серии \n","    statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel) #сгенерируем игровой эпизод\n","    batchStateList.extend(statesList) #добавим список состояний в серию списков состояний\n","    batchNetworkOutputList.extend(networkOutputList) #добавим список вероятностей в серию списков из вероятности идти вверх\n","    batchUpDownActionList.extend(upDownActionList) #добавим список движений в серию списков движений\n","    batchRewardsList.extend(rewardsList) # добавим список наград в серию списков наград\n","\n","  episodeReward = np.expand_dims(processRewards(batchRewardsList), 1) #зададим награды и изменим форму массива с добавлением оси\n","  x = np.array(batchStateList) #сформируем массив из серии списков состояний в качестве 'x' для нейросети\n","  yTmp = np.array(batchUpDownActionList) #зададим 'y' как серию из списков движений вверх(1)/вниз(0)\n","  yTrue = np.expand_dims(yTmp, 1) #подгоним форму 'y' массива с добавлением оси\n","  \n","  history = policyNetworkTrain.fit(x=[x,episodeReward], y=yTrue, epochs=5, verbose=0) #скормим нейросети серию 'x'ов и 'y'ов\n","  \n","  batchLoss = history.history['loss'][-1]\n","  return batchStateList, batchUpDownActionList, batchRewardsList, batchNetworkOutputList, batchLoss\n","  #функция вернёт серию списков состояний, серию списков движений, серию списков наград, серию списков из вероятности идти вверх"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOGjuR96Oukg"},"source":["## Достижение победы (99 эпох)"]},{"cell_type":"code","metadata":{"id":"C0K_xSfwkicT","executionInfo":{"status":"aborted","timestamp":1626155134410,"user_tz":-180,"elapsed":32,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["trainingTimes = 99 #установим количество тренировок \n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bE48IL6UordI"},"source":["Победы на 99 эпохах достичь не удалось. Максимально 108 очков на 86м эпизоде (тренировке)"]},{"cell_type":"markdown","metadata":{"id":"I1-2tvGqNSeP"},"source":["### 10 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"k-TJ5Hk0nEcF","executionInfo":{"status":"aborted","timestamp":1626155134410,"user_tz":-180,"elapsed":31,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["trainingTimes = 10 #установим количество тренировок \n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69wXYuHqH-Nw"},"source":["Вывод: постепенное улучшение результатов\n"]},{"cell_type":"markdown","metadata":{"id":"RLxe5RX0NXgm"},"source":["### 20 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"YFZdyOMcnIPn","executionInfo":{"status":"aborted","timestamp":1626155134411,"user_tz":-180,"elapsed":32,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["trainingTimes = 10 #установим количество тренировок \n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fX8FVF_kH4cA"},"source":["Вывод: 50 забитых очков. Прогресс"]},{"cell_type":"markdown","metadata":{"id":"1q97oeucNYzQ"},"source":["### 30 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"jAhcoa4hnJIZ","executionInfo":{"status":"aborted","timestamp":1626155134411,"user_tz":-180,"elapsed":32,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["trainingTimes = 10 #установим количество тренировок \n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNWUM4KSHvdt"},"source":["Вывод: на 30 тренировке заметный прогресс -  60 забитых очков. \n","Побед нет."]},{"cell_type":"markdown","metadata":{"id":"AajzftiKybYf"},"source":["## --------------------\n","Сыграем эпизод обученной моделью и воспроизведем видео игры"]},{"cell_type":"code","metadata":{"id":"uchGOlfyyXwl","executionInfo":{"status":"aborted","timestamp":1626155134415,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Импорт библиотек для записи и воспроизведения видео\n","###########################\n","!pip install pyvirtualdisplay > /dev/null 2>&1 \n","# Устанавливаем виртуальный дисплей pyvirtualdisplay; \"/dev/null 2>&1\" уберёт длинный вывод в строке output\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1 #создает вирт.дисплей в памяти(нужен для работы дисплея в ноутбуке)\n","# \"apt-get...\" - установка пакета программного обеспечения\n","# \"...-y...\" автоматический \"yes\" на вопрос \"вы действительно хотите это установить?\" \n","# \"...xvfb...\" - бэкенд для pyvirtualdisplay - сервер, который выполняет все графические операции в памяти без вывода на экран\n","# \"...python-opengl...\" - поддержка графики с помощью графической библиотеки\n","# \"...ffmpeg\" - пакеты для обработки/конвертирования видеофайла из одного формата в другой\n","\n","from gym.wrappers import Monitor #класс Monitor из пакета функций-обёрток в gym активирует видеозапись игры\n","import glob #модуль возвращает список путей к видео по шаблону (для удобства поиска/воспроизведения текущего видео)\n","import base64 #библиотека поможет нам закодировать видео в 64-разрядный код, и без повреждений/изменений открыть его в ноутбуке через HTML\n","\n","from IPython.display import HTML #загружаем модуль чтобы обратиться к HTML для открытия закодированного видео \n","from IPython import display as ipythondisplay #активирует дисплей для воспроизведения видео в интерфейсе ноутбука\n","from pyvirtualdisplay import Display #модуль для активации виртуального дисплея\n","display = Display(visible=0, size=(1400, 900)) #запустим невидимый виртуальный дисплей\n","display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSA9O3aCynx9","executionInfo":{"status":"aborted","timestamp":1626155134415,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Задаем функции записи и воспроизведения видео\n","###########################\n","def wrapEnv(env): #зададим функцию-обёртку над средой\n","  env = Monitor(env, './video', force=True) # класс Monitor будет записывать игру в видеофайл\n","  return env #теперь вызов среды через функцию wrapEnv будет запускать видеозапись\n","\n","def showVideo(): #функция для воспроизведения видео в ноутбуке\n","  mp4list = glob.glob('video/*.mp4') #возвращает список путей к видео по этому шаблону\n","  if len(mp4list) > 0: #если по этому пути нашелся хотя бы один файл\n","    mp4 = mp4list[0] #то берём самый свежий файл\n","    video = open(mp4, 'r+b').read() #открываем в режиме чтения/записи бинарного файла\n","    encoded = base64.b64encode(video) #кодируем видео в 64-разрядный код \n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    # Декодируем ascii формат в видео mp4 - \"format(encoded.decode('ascii'))))\"\n","    # для воспроизведения на разных браузерах - \"<source src=\"data:video/mp4\"\n","    # задаём высоту экрана 400px - \"height: 400px;\"\n","    # активируем элементы управления в видео(типа кнопки паузы и т.п) - \"controls\"\n","    # включаем автоповтор после окончания воспроизведения - \"loop\"\n","    # видео автоматически воспроизведется по готовности - \"autoplay\"\n","    # текстовая альтернатива описывающая объект в случае невозможности отображения - \"alt=\"test\"\"\n","    \n","  else: #если путь не нашелся \n","    print(\"Could not find video\") #то выведем на печать, что не смог найти видео"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4q1cWKjtutp","executionInfo":{"status":"aborted","timestamp":1626155134416,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["###########################\n","# Задаем функцию сыграть и показать игру\n","###########################\n","def playAndShowEpisode(policyNetwork): #подаем на вход функции модель нейросети\n","    env = wrapEnv(gym.make('Pong-v0')) #создаем среду в режиме записи\n","    done = False #игровой эпизод не завершён\n","    observation = env.reset() #задаем начальное состояние среды, которое наблюдает агент\n","    newObservation = observation #задали новое состояние которое наблюдает агент\n","    while done == False: #пока игровой эпизод не завершён\n","        # На вход сети будет подаваться очередное состояние - разница между кадрами. \n","        processedNetworkInput = preprocessFrames(newFrame=newObservation,lastFrame=observation) #зададим это состояние\n","        reshapedInput = np.expand_dims(processedNetworkInput,axis=0) #размер 'x' - (80,80), делаем размерность (x,(1,80,80))\n","\n","        upProbability = policyNetwork.predict(reshapedInput,batch_size=1)[0][0] #задаем вероятность шага вверх\n","        actualAction = np.random.choice(a=[2,3], size=1, p=[upProbability,1-upProbability])\n","        # сделаем фактический шаг либо вверх(2) c вероятностью upProbability, либо вниз(3) с обратной вероятностью\n","        \n","        env.render() #запускаем воспроизведение среды\n","        \n","        observation = newObservation #текущий newObservation записываем как старый, перед тем как сделать следующий шаг\n","        newObservation, reward, done, info = env.step(actualAction) #сделали новый шаг, получили новую награду, новое состояние\n","\n","    env.close() #закрываем игровую среду после окончания игры\n","    showVideo() #показать видео игры"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UlPhDJbtxps","executionInfo":{"status":"aborted","timestamp":1626155134416,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["playAndShowEpisode(policyNetworkModel) #запускаем игровой эпизод c необученной моделью нейросети"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xZOI-DXM0Lf","executionInfo":{"status":"aborted","timestamp":1626155134417,"user_tz":-180,"elapsed":35,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["#взглянем как сыграет сетка, обученная на 30 тренировках\n","policyNetworkModel30 = load_model(\"policyNetworkModel30.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel30) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9S7aXBoMsOt","executionInfo":{"status":"aborted","timestamp":1626155134417,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["#взглянем как сыграет сетка, обученная на 60 тренировках\n","policyNetworkModel60 = load_model(\"policyNetworkModel60.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel60) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw8V8TK7ikPF","executionInfo":{"status":"aborted","timestamp":1626155134417,"user_tz":-180,"elapsed":34,"user":{"displayName":"Mikhail Zaytsev","photoUrl":"","userId":"11256894357127576886"}}},"source":["#взглянем как сыграет сетка, обученная на 120 тренировках\n","policyNetworkModel120 = load_model(\"policyNetworkModel120.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel120) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]}]}